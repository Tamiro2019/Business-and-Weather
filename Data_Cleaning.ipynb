{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook details the process carried to obtain the datasets 'yb50.csv' and 'ts50.csv' which are used in the Business-and-Weather project.\n",
    "\n",
    "The core business data is obtained from the [Yelp Dataset](https://www.yelp.com/dataset). Yelp data spans several states; below we will focus on data from Pittsburgh, Pennsylvania. \n",
    "\n",
    "Weather data from a land-based station in the Pittsburgh International Airport was obtained from the [National Centers for Environmental Information](https://www.ncdc.noaa.gov/data-access/).\n",
    "\n",
    "The data is used here for personal and educational purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import json\n",
    "\n",
    "from datetime import timedelta, date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'yelp_dataset/business.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-388838f9f548>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read .json files into lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yelp_dataset/business.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yelp_dataset/checkin.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# use pandas to build dataframes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yelp_dataset/business.json'"
     ]
    }
   ],
   "source": [
    "# read .json files into lists\n",
    "data_b = [json.loads(line) for line in open('yelp_dataset/business.json','r')]\n",
    "data_c = [json.loads(line) for line in open('yelp_dataset/checkin.json','r')]\n",
    "\n",
    "# use pandas to build dataframes\n",
    "df_b = pd.DataFrame(data_b) # for business info\n",
    "df_c = pd.DataFrame(data_c) # for check-in info\n",
    "df_w = pd.read_csv('weather_dataset/weather_data.csv',dtype=object) # for weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets process the business and check_in datasets to exact the information most relevant to this project: business names, categories, and daily check-in counts (for businesses in Pittburgh)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Pittburg business data, and sort by business_id:\n",
    "todrop = ['state','attributes','city','postal_code','latitude','longitude','is_open','hours','address','review_count','stars']\n",
    "b_pit = df_b[df_b['city'].values=='Pittsburgh'].sort_values(by=['business_id'])\n",
    "\n",
    "# Drop irrelevant columns:\n",
    "b_pit.drop(todrop,axis=1,inplace=True)\n",
    "\n",
    "# Get Pittburg check-in data for only those businesses in b_pit, and sort by business_id:\n",
    "b_ids = b_pit['business_id']\n",
    "c_pit = df_c[df_c['business_id'].isin(b_ids.tolist())].sort_values(by=['business_id'])\n",
    "\n",
    "# Rename column named 'date' to 'check_in'.\n",
    "c_pit.rename({'date':'check_in'}, axis=1,inplace=True)\n",
    "\n",
    "# Merge the b_pit and c_pit into a single dataframe:\n",
    "df = pd.merge(b_pit,c_pit, on= 'business_id')\n",
    "\n",
    "# Clean up business indices\n",
    "df['business_id'] = np.arange(0,len(df.index))\n",
    "df = df.set_index(['business_id'])\n",
    "\n",
    "# Add aggregate check-in count column\n",
    "df['total_count'] = df['check_in'].str.split(', ').apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the data reveals that there are in fact duplicates of certain business names like 'Starbucks', which have stores different locations. We want to combine these into a single entry under the same name, aggregating information about check-in counts.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the check-in datetime events\n",
    "def combine_ci(x):\n",
    "    ci_list = []\n",
    "    for elem in x:\n",
    "        ci_list.extend(str.split(elem,', '))\n",
    "        \n",
    "    return ci_list    \n",
    "\n",
    "# Group by common names and then aggregate by \n",
    "# - selecting the smallest business id and the longest cateory description \n",
    "# - combining the check-in events lists and summing their total count.\n",
    "df = df.reset_index().groupby('name').agg({'business_id': 'min' ,'categories': lambda x : max(x.astype(str), key=len),'check_in' : combine_ci,'total_count': 'sum'})\n",
    "\n",
    "# Restate index\n",
    "df.sort_values(by='business_id',inplace=True)#.set_index(['business_id'])\n",
    "df = df.reset_index().set_index(['business_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We shall pick the 50 businesses with highest total check-in counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top businesses by total number of check-ins.\n",
    "\n",
    "top = 50 # number of businesses we shall select\n",
    "\n",
    "# Get the indices of the 50 top businesses\n",
    "top_ids = df['total_count'].sort_values(ascending=False).index.values[:top] \n",
    "\n",
    "# Select out top businesses\n",
    "df50 = df[df.index.isin(top_ids)].reset_index(drop=True)\n",
    "\n",
    "print(df50.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduce the data to a particular range of dates: Jan 2014 through Dec 2018. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create iterable range of dates.\n",
    "def daterange(start_date, end_date): \n",
    "    diff = end_date - start_date \n",
    "    for n in range(int(diff.days)): \n",
    "        yield start_date + timedelta(days = n)\n",
    "\n",
    "# Rounds datetime to the nearest day\n",
    "def round_day(t):\n",
    "    return t.replace(second=0, minute=0,hour=0)\n",
    "        \n",
    "# Convert datetime string into datetime (to the nearest day).\n",
    "def convert(sdt):\n",
    "    date = round_day(dt.datetime.strptime(sdt, '%Y-%m-%d %H:%M:%S'))\n",
    "    return date\n",
    "\n",
    "# Convert list of datetime strings into a list of datetimes (to the nearest day).\n",
    "def convert_list(ci_list):\n",
    "    dates_list = [round_day(dt.datetime.strptime(date, '%Y-%m-%d %H:%M:%S')) for date in ci_list]\n",
    "    return dates_list\n",
    "\n",
    "# Count the number of occurences of a single datetime inside a datetime list.\n",
    "def count(dt_list, sdt = None):\n",
    "    return dt_list.count(sdt) \n",
    "        \n",
    "# Set datetime range\n",
    "start_datetime = dt.datetime(2015, 1, 1) \n",
    "end_datetime = dt.datetime(2019, 1, 1) \n",
    "\n",
    "# Construct a new column for every daily count of check-ins\n",
    "ts1 = (df50['check_in'].apply(convert_list)) # auxiliary function outside loop for efficiency\n",
    "for single_datetime in daterange(start_datetime, end_datetime): \n",
    "    datestamp = single_datetime.date()\n",
    "    df50[datestamp] = ts1.apply(count, sdt = single_datetime) \n",
    "\n",
    "# Obtain time-indexed daily check-in count across the 50 businesses\n",
    "ts1 = df50.drop(['name','categories','check_in','total_count'],axis=1).T ;\n",
    "ts1.index.name = 'Date'\n",
    "\n",
    "ts1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up their datetime notation:\n",
    "def nicedt(uglydt):\n",
    "    return ' '.join(uglydt.split('T'))\n",
    "\n",
    "# Pick Columns of Interest\n",
    "ts2 = df_w[['DATE','HourlyDryBulbTemperature','HourlyPrecipitation','HourlyWindSpeed']].copy()\n",
    "\n",
    "# Get clear datetime notation and type\n",
    "ts2['DATE'] = pd.to_datetime(ts2['DATE'].apply(nicedt))\n",
    "\n",
    "# Give cleaner names to the variables\n",
    "ts2.columns = ['Date', 'T', 'P','WS']\n",
    "\n",
    "# Round the hours \n",
    "ts2['Date'] = ts2['Date'].apply(round_day)\n",
    "\n",
    "# Select the date range\n",
    "ts2 = ts2[ (ts2['Date'] >= start_datetime) & (ts2['Date'] < end_datetime)]\n",
    "\n",
    "# Clean weather data. There are extra notational 's' in the table to emphasize uncertain measurements. \n",
    "# We will include values even if only approximate.\n",
    "\n",
    "ts2['T'] = ts2['T'].str.strip('s')\n",
    "ts2['P'] = ts2['P'].str.strip('s')\n",
    "ts2['WS'] = ts2['WS'].str.strip('s')\n",
    "\n",
    "# Set trace precipitation values to zero\n",
    "ts2['P'].replace('T','0.00',inplace=True)\n",
    "\n",
    "\n",
    "## Fill Missing Reported Datetimes ##\n",
    "\n",
    "# Replace * missing values\n",
    "ts2['T'].replace('*',np.NaN,inplace=True)\n",
    "\n",
    "# Get set of unique dates : for loop below\n",
    "date_set = set(ts2['Date']) \n",
    "\n",
    "# Set the date as index\n",
    "ts2 = ts2.set_index('Date') \n",
    "\n",
    "counter = 0\n",
    "for datetime in daterange(start_datetime,end_datetime):\n",
    "    if datetime in date_set:\n",
    "        pass\n",
    "    else:    \n",
    "        counter += 1\n",
    "        ts2 = ts2.append(pd.Series(name=datetime)) # add the datetime\n",
    "        ts2 = ts2.sort_index() # sort the datetimes\n",
    "        #print(datetime, 'was missing and was added with forward filling') # forward filling below\n",
    "\n",
    "print('missing hours:',counter)\n",
    "\n",
    "#Turn into numeric type and fill in missing values (forward/backward filling)\n",
    "ts2 = ts2.astype(float)\n",
    "ts2 = ts2.fillna(method ='ffill')\n",
    "ts2 = ts2.fillna(method ='bfill')\n",
    "\n",
    "# Data has different sources and report types, we will average duplicate datetimes.\n",
    "\n",
    "# Use groupby and mean to remove duplicate dates.\n",
    "ts2 = ts2.reset_index().groupby(['Date']).mean().reset_index()\n",
    "\n",
    "# Check no redundant temperatures \n",
    "print('# idx values:',len(set(df_w.index.values)),'# of indices:', len(df_w.index))\n",
    "\n",
    "# Round values\n",
    "ts2[['T','WS']] = ts2[['T','WS']].round(1)\n",
    "ts2['P'] = ts2['P'].round(2)\n",
    "\n",
    "ts2 = ts2.set_index('Date')\n",
    "\n",
    "ts2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join business and weather frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('business days =', len(ts1.index))\n",
    "print('weather days =', len(ts2.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts50 = ts1.join(ts2)\n",
    "ts50.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write datasets to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df50[['name','categories']].to_csv('yb50.csv')\n",
    "ts50.to_csv('ts50.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
